{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático 2\n",
    "**Disciplina:** Inteligência Artificial (COMP0427)<br>\n",
    "**Turma:** T01 - 2018.2<br>\n",
    "**Orientador:** Prof. Dr. Hendrik Teixeira Macedo<br>\n",
    "**Equipe:** *Ontological Entity*<br>\n",
    "**Integrantes:** Airton Matheus Cardoso Leite, Eric Rocha Soares, Hugo Vinicius Dantas Barreto,<br>Igor de Figueiredo Rodrigues, Lucas Brabec Barreto Santana e Tiago Conceição dos Santos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Descrição do problema\n",
    "\n",
    "O web site oficial da ferramenta Netica da Norsys disponibiliza diversas redes bayesianas já definidas para download (www.norsys.com/netlibrary/index.htm) em diversas categorias de conhecimento especialista: Ambiental, Médico, Financeiro/Gerencial, Geológico, Diagnóstico Industrial e Outros Industriais. Para cada uma dessas categorias, existem várias redes de exemplo com respectiva descrição da fonte explicativa do problema.\n",
    "\n",
    "A missão é escolher uma dessas redes e proceder com um experimento em Aprendizado de Máquina, descrito a seguir:<br>\n",
    "**1-** Eleger uma característica de classe;<br>\n",
    "**2-** Utilizar técnica de amostragem em redes bayesianas para produzir um dataset de exemplos;<br>\n",
    "**3-** Reservar parte do dataset para treinamento e parte para testes;<br>\n",
    "**4-** Treinar modelo Naïve Bayes;<br>\n",
    "**5-** Treinar modelo de Árvores de Decisão (ou Floresta Aleatória);<br>\n",
    "**6-** Testar modelos com o conjunto de testes;<br>\n",
    "**7-** Responder a algumas perguntas: O quanto cada modelo se aproxima da rede bayesiana original? O quanto o tamanho do dataset gerado influencia na qualidade? Smoothing se faz necessário? Quem possui melhor comportamento: Naïve Bayes ou Árvore de Decisão?<br>\n",
    "**8-** Concluir o experimento.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Solução adotada\n",
    "\n",
    "A rede de crença bayesiana adotada, relativa ao conhecimento especialista na área médica de Pneumologia, é a que segue:<br>\n",
    "\n",
    "![*](Chest_Clinic_Bayesian_Network.png)<br>\n",
    "\n",
    "<br>É uma versão simplificada de uma rede que poderia ser usada para diagnosticar pacientes que chegam a uma clínica. Cada nó na rede corresponde a alguma condição do paciente. Os dois nós superiores são para predisposições que influenciam a probabilidade das doenças. Essas doenças aparecem na linha abaixo deles. Na parte inferior são sintomas das doenças. Em grande medida, os elos da rede correspondem a causação. Os links entre os nós indicam como os relacionamentos entre os nós são estruturados. Essa é uma estrutura comum para redes de diagnóstico: nós de predisposição no topo, com links para nós representando condições internas e estados de falha, que por sua vez possuem links para nós para evidenciáveis.\n",
    "\n",
    "**1-** A característica de classe adotada foi a variável aleatória ***Tuberculosis or Cancer***, por seu tom mais dramático.\n",
    "\n",
    "**2-** A técnica de amostragem implementada foi a ***Prior Sampling***, gerando um *dataset* inicial com 100 mil *samples*.<br>Essa abordagem é melhor explorada na seção 3.\n",
    "\n",
    "**3-** Foi implementado script de *splitting* do dataset inicial em 66,6% para *training dataset* e 33,4% para *testing dataset*.<br>Essa abordagem é melhor explorada na seção 4.\n",
    "\n",
    "**4-** Um modelo ***Naïve Bayes*** *baseado em distribuião gaussiana* foi inicialmente adotado. Porém a intenção é ajustá-lo para um classificador de modelos multivariados de Bernoulli, devido à natureza booleana das probabilidades da classe. Essa abordagem é melhor explorada na seção 5.\n",
    "\n",
    "**5-** Um modelo de Floresta de Decisão Aleatória com Árvores de Decisão ensacadas com variância. Essa abordagem é melhor explorada na seção 6.\n",
    "\n",
    "**6-** Ambos os modelos foram testados com respectivos testing datasets de 10 mil, 100 mil e 1 milhão de samples. Cada bateria de testes segue nas seções 7 e 8, para *Naïve Bayes* e *Random Decision Forest* respectivamente.\n",
    "\n",
    "**7-** Uma breve discussão dos resultados dos testes é discorrida na seção 9.\n",
    "\n",
    "**8-** Nossas considerações finais encerram este projeto na seção 10, seguidas pelas Referências bibliográficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Amostragem de dados\n",
    "\n",
    "Os modelos probabilísticos usados são geralmente bem complexos e bastante esforço de pesquisa científica em ML é gasto no desenvolvimento de algoritmos que geram soluções aproximadas para o problema de inferência. Um método de amostragem produz respostas ao gerar repetidamente números aleatórios a partir de uma distribuição de interesse e pode ser usado para realizar consultas de inferência marginais ou de probabilidade máxima a posteriori.\n",
    "\n",
    "O processo de amostragem prévia (Prior Sampling) é consistente e gera amostras com probabilidade: **¹Πⁿ P( Xi | Pais(Xi) )**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A node in the bayes network\n",
    "class Node:\n",
    "    def __init__(self, n, pa, pr):\n",
    "        self.name = n\n",
    "        self.parents = pa\n",
    "        self.probs = pr\n",
    "        self.value = None\n",
    "\n",
    "    '''\n",
    "    Returns conditional probability of value \"true\" for the current\n",
    "    node based on the values of the parent node/s.\n",
    "    '''\n",
    "    def conditionalProbability(self):\n",
    "        index = 0\n",
    "\n",
    "        for i, p in enumerate(self.parents):\n",
    "            if p.value == False:\n",
    "                index += 2 ** (len(self.parents) - i - 1)\n",
    "        return self.probs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concluido.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "PLAYING_EXAMPLES = [(True, False), (True, False), (True, True),\n",
    "                    (True, False), (True, False), (False, True),\n",
    "                    (False, False), (False, True), (False, True),\n",
    "                    (False, True), (False, False), (False, True),\n",
    "                    (False, True)]\n",
    "\n",
    "class BayesNet:\n",
    "    # The nodes in the network\n",
    "    nodes = []\n",
    "\n",
    "    # Build the initial network\n",
    "    def __init__(self):\n",
    "        self.nodes.append(Node(\"Visit to Asia\", [], [0.01])) #index=0\n",
    "        self.nodes.append(Node(\"Smoking\", [], [0.5])) #index=1\n",
    "\n",
    "\n",
    "        '''\n",
    "                Visit Asia+ |   Visit asia-\n",
    "            T    0.5                0.1\n",
    "        '''\n",
    "        self.nodes.append(Node(\"Tuberculosis\",\n",
    "                               [self.nodes[0]],\n",
    "                               [0.5, 0.1])) #index=2\n",
    "\n",
    "        '''\n",
    "                Lung Cancer+ |   Lung Cancer-\n",
    "        Smoking    0.10                0.01\n",
    "        '''\n",
    "        self.nodes.append(Node(\"Lung Cancer\",\n",
    "                               [self.nodes[1]],\n",
    "                               [0.1, 0.01])) #index=3\n",
    "        '''\n",
    "                Bronchitis+ |   Bronchitis-\n",
    "        Smoking    0.6                0.3\n",
    "        '''\n",
    "        self.nodes.append(Node(\"Bronchitis\",\n",
    "                               [self.nodes[1]],\n",
    "                               [0.6, 0.3])) #index=4\n",
    "        '''\n",
    "            Tuberculosis | Lung Cancer | Tuberculosis or Cancer = True |  Tuberculosis or Cancer = False\n",
    "                True            True                    1                               0\n",
    "                True            False                   1                               0\n",
    "                False           True                    1                               0\n",
    "                False           False                   0                               1\n",
    "\n",
    "        '''\n",
    "        self.nodes.append(Node(\"Tuberculosis or Cancer\",\n",
    "                               [self.nodes[2], self.nodes[3]],\n",
    "                               [1, 1, 1, 0])) #index=5\n",
    "\n",
    "\n",
    "        '''\n",
    "                Tuberculosis or Cancer+ | Tuberculosis or Cancer-\n",
    "         XRay           0.98                        0.05\n",
    "        '''\n",
    "        self.nodes.append(Node(\"X-Ray Result\", [self.nodes[5]], [0.98, 0.05])) #index=6\n",
    "\n",
    "        '''\n",
    "                    Tuberculosis or Cancer | Bronchitis | Dyspnea=True | Dyspnea=False\n",
    "                            True            True               0.90             0.1\n",
    "                            True            False              0.70             0.3\n",
    "                            False           True               0.80             0.2\n",
    "                            False           False              0.10             0.90\n",
    "\n",
    "\n",
    "        '''\n",
    "        self.nodes.append(Node(\"Dyspnea\", [self.nodes[5], self.nodes[4]],\n",
    "                          [0.90,0.70,0.80,0.10]))\n",
    "        #self.nodes.append(self.nodes.pop(5))\n",
    "\n",
    "    # Prints the current state of the network to stdout\n",
    "    def printState(self):\n",
    "        strings = []\n",
    "        for node in self.nodes:\n",
    "            strings.append(node.name + \" = \" + str(node.value))\n",
    "\n",
    "        print(\", \".join(strings))\n",
    "\n",
    "    def calculatePlayOutsideProbabilities(self, rainingInstances):\n",
    "        playing = [0, 0]\n",
    "        total = [0, 0]\n",
    "        prob = [0.0, 0.0]\n",
    "\n",
    "        for sample in rainingInstances:\n",
    "            if sample[0]:\n",
    "                playing[0] += 1 if sample[1] else 0\n",
    "                total[0] += 1\n",
    "            else:\n",
    "                playing[1] += 1 if sample[1] else 0\n",
    "                total[1] += 1\n",
    "\n",
    "        prob[0] = float(playing[0]) / float(total[0])\n",
    "        prob[1] = float(playing[1]) / float(total[1])\n",
    "\n",
    "        return prob\n",
    "\n",
    "    '''\n",
    "    This method will sample the value for a node given its\n",
    "    conditional probability.\n",
    "    '''\n",
    "    def sampleNode(self, node):\n",
    "        node.value = True if random.random() <= node.conditionalProbability() else False\n",
    "\n",
    "    '''\n",
    "    This method assigns new values to the nodes in the network by\n",
    "    sampling from the joint distribution.  Based on the PRIOR-SAMPLE\n",
    "    from the text book/slides\n",
    "    '''\n",
    "    def priorSample(self):\n",
    "        for n in self.nodes:\n",
    "            self.sampleNode(n)\n",
    "\n",
    "        return self.nodes\n",
    "    '''\n",
    "    This method will return true if all the evidence variables in the\n",
    "    network have the value specified by the evidence values.\n",
    "    '''\n",
    "    def testModel(self, indicesOfEvidenceNodes, evidenceValues):\n",
    "        for i in range(len(indicesOfEvidenceNodes)):\n",
    "            if (self.nodes[indicesOfEvidenceNodes[i]].value != evidenceValues[i]):\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def printState(self):\n",
    "        strings = []\n",
    "        for node in self.nodes:\n",
    "            strings.append(node.name + \" = \" + str(node.value))\n",
    "\n",
    "        print(\", \".join(strings))\n",
    "\n",
    "    '''\n",
    "        this method receives an number n of samples that will generate with prior sampling\n",
    "        the result is an array of maps consisting of key values\n",
    "        EX:\n",
    "        [{Visit to Asia:False},{Smoking:False},{Tuberculosis:False},{Lung Cancer:False},{Bronchitis:True},{Tuberculosis or Cancer:False},{X-Ray Result:False},{Dyspnea:True}]\n",
    "    '''\n",
    "    def getSamples(self, n):\n",
    "        result = []\n",
    "        sublist = []\n",
    "        for i in range(n):\n",
    "            for node in self.priorSample():\n",
    "                sublist.append({node.name: node.value})\n",
    "\n",
    "            result.append(sublist)\n",
    "            #self.printState()\n",
    "\n",
    "        return result\n",
    "\n",
    "    '''\n",
    "        this method receives an number n of samples that will generate with prior sampling\n",
    "        the result is file named sampling.txt created in the folder of tthe project with strings consisting of key values\n",
    "        EX:\n",
    "        {Visit to Asia:False},{Smoking:False},{Tuberculosis:False},{Lung Cancer:False},{Bronchitis:True},{Tuberculosis or Cancer:False},{X-Ray Result:False},{Dyspnea:True}\n",
    "\n",
    "    '''\n",
    "    def beginSamplingAndSaveToFile(self,numberOfSamples, filename):\n",
    "        result = []\n",
    "        \n",
    "        for sample in range(len(numberOfSamples)):\n",
    "            with open(filename+\"-naive\"+str(sample)+\".csv\", 'w'):\n",
    "                    pass\n",
    "            with open(filename+\"-tree\"+str(sample)+\".csv\", 'w'):\n",
    "                    pass\n",
    "            for i in range(numberOfSamples[sample]):\n",
    "                strings = []\n",
    "                strings2 = []\n",
    "                for node in self.priorSample():\n",
    "                    #strings.append(node.name + \":\" + str(node.value)) #versão para naive bayes\n",
    "                    strings2.append('1' if node.value == True else '0') # versão para floresta aleatoria\n",
    "                \n",
    "                #strings2.reverse()\n",
    "                self.appendStringToFile(filename +\"-naive\" + str(sample) + \".csv\", \",\".join(strings2))\n",
    "                strings2.reverse()\n",
    "                self.appendStringToFile(filename +\"-tree\" + str(sample)+ \".csv\", \",\".join(strings2))\n",
    "\n",
    "                #self.printState()\n",
    "                \n",
    "        return result\n",
    "\n",
    "    # helper method that gets a file name and a string and saves the string to the file\n",
    "    def appendStringToFile(self, filename, input):\n",
    "        with open(filename, 'a') as file:\n",
    "                file.write(input + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    b = BayesNet() # Creates a bayes net\n",
    "    # parameters of how many examples in each of the three sample archives generated for each method (six samples are generated)\n",
    "    numberOfSamples = [100,1000,10000]\n",
    "    nodes = b.beginSamplingAndSaveToFile(numberOfSamples,\"sample\")\n",
    "    print(\"Concluido.\")\n",
    "    '''\n",
    "    strings = []\n",
    "    for node in nodes:\n",
    "        for colums in node:\n",
    "            strings.append(colums)\n",
    "\n",
    "    print(strings)\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset splitting\n",
    "\n",
    "É padrão em ML se dividir o conjunto de dados em subconjuntos de treinamento e teste. A razão para isso é que tentar avaliar o agente de aprendizado com dados já treinados é algo irrealista, o objetivo de um agente supervisionado é ser capaz de classificar dados previamente desconhecidos.\n",
    "\n",
    "* *Esse script foi criado antes de notar que as classes de Naïve Bayes e Random Decision Forest têm funções de split próprias.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples =  10000\n",
      "Number of training examples =  6660\n",
      "Number of testing examples =  3340\n"
     ]
    }
   ],
   "source": [
    "# program to split the sample into training and testing datasets\n",
    "import sys\n",
    "\n",
    "# opening the files\n",
    "sample = open('sample-naive2.csv','r')\n",
    "#sample = open(sys.argv[1], \"r\")\n",
    "\n",
    "training_dataset = open('./datasets/training_dataset.csv','w')\n",
    "testing_dataset = open('./datasets/testing_dataset.csv','w')\n",
    "\n",
    "\n",
    "sample_number = len(sample.readlines())\n",
    "print('Total number of examples = ', sample_number)\n",
    "\n",
    "training_cut_parameter = int(sample_number * 0.666)\n",
    "print('Number of training examples = ', training_cut_parameter)\n",
    "testing_cut_parameter = int(sample_number * 0.334)\n",
    "print('Number of testing examples = ', testing_cut_parameter)\n",
    "\n",
    "# return to the sample file start\n",
    "sample.seek(0)\n",
    "\n",
    "# splitting the sample into 66,6% training dataset\n",
    "for example in range(training_cut_parameter):\n",
    "    training_dataset.write(sample.readline())\n",
    "\n",
    "# and 33,4% testing dataset\n",
    "for example in range(testing_cut_parameter):\n",
    "    testing_dataset.write(sample.readline())\n",
    "\n",
    "# safely closing the files\n",
    "sample.close()\n",
    "training_dataset.close()\n",
    "testing_dataset.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Naïve Bayes\n",
    "\n",
    "O algoritmo *Naïve Bayes* é um classificador probabilístico baseado no **Teorema de Bayes** que desconsidera a correlação entre variáveis, tratando cada fator de forma independente. E leva em consideração apenas a relação de probabilidade condicional a posteriori de cada característica com o atributo de classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0 0.0 -0.0 0.0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-d6c496764593>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunBayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-d6c496764593>\u001b[0m in \u001b[0;36mrunBayes\u001b[1;34m(i)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[0mtrainingSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplitDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[0msummaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msummarizeByClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetPredictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-d6c496764593>\u001b[0m in \u001b[0;36mgetPredictions\u001b[1;34m(summaries, testSet)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestSet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m                 \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-d6c496764593>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(summaries, inputVector)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputVector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mprobabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculateClassProbabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputVector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[0mbestLabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbestProb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mclassValue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobability\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-d6c496764593>\u001b[0m in \u001b[0;36mcalculateClassProbabilities\u001b[1;34m(summaries, inputVector)\u001b[0m\n\u001b[0;32m     65\u001b[0m                         \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassSummaries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputVector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                         \u001b[0mprobabilities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclassValue\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcalculateProbability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-d6c496764593>\u001b[0m in \u001b[0;36mcalculateProbability\u001b[1;34m(x, mean, stdev)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m#exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(1 ,2))))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstdev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstdev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mexponent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstdev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;31m#return (1 / (math.sqrt(2*math.pi) * stdev)) * exponent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstdev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mexponent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import math\n",
    "\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "def loadCsv(filename):\n",
    "    lines = csv.reader(open(filename, \"r\"))\n",
    "    dataset = list(lines)\n",
    "    for i in range(len(dataset)):\n",
    "        #dataset[i] = [str(x).split(':')[1] == 'True' for x in dataset[i]]\n",
    "        dataset[i] = [int(x) for x in dataset[i]]\n",
    "    #print(dataset)\n",
    "    return dataset\n",
    " \n",
    "def splitDataset(dataset, splitRatio):\n",
    "\ttrainSize = int(len(dataset) * splitRatio)\n",
    "\ttrainSet = []\n",
    "\tcopy = list(dataset)\n",
    "\twhile len(trainSet) < trainSize:\n",
    "\t\tindex = random.randrange(len(copy))\n",
    "\t\ttrainSet.append(copy.pop(index))\n",
    "\treturn [trainSet, copy]\n",
    " \n",
    "def separateByClass(dataset):\n",
    "\tseparated = {}\n",
    "\tfor i in range(len(dataset)):\n",
    "\t\tvector = dataset[i]\n",
    "\t\tif (vector[-1] not in separated):\n",
    "\t\t\tseparated[vector[-1]] = []\n",
    "\t\tseparated[vector[-1]].append(vector)\n",
    "\treturn separated\n",
    " \n",
    "def mean(numbers):\n",
    "\treturn sum(numbers)/float(len(numbers))\n",
    " \n",
    "def stdev(numbers):\n",
    "\tavg = mean(numbers)\n",
    "\tvariance = sum([pow(x-avg,2) for x in numbers])/float(len(numbers)-1)\n",
    "\treturn math.sqrt(variance)\n",
    " \n",
    "def summarize(dataset):\n",
    "\tsummaries = [(mean(attribute), stdev(attribute)) for attribute in zip(*dataset)]\n",
    "\tdel summaries[-1]\n",
    "\treturn summaries\n",
    " \n",
    "def summarizeByClass(dataset):\n",
    "\tseparated = separateByClass(dataset)\n",
    "\tsummaries = {}\n",
    "\tfor classValue, instances in separated.items():\n",
    "\t\tsummaries[classValue] = summarize(instances)\n",
    "\treturn summaries\n",
    "\n",
    "def calculateProbability(x, mean, stdev):\n",
    "    #exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(1 ,2))))\n",
    "    print(x,mean,stdev,-1*math.pow(x-mean,2),2*math.pow(stdev,2))\n",
    "    exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))\n",
    "    #return (1 / (math.sqrt(2*math.pi) * stdev)) * exponent\n",
    "    return (1 / (math.sqrt(2*math.pi) * stdev)) * exponent\n",
    " \n",
    "def calculateClassProbabilities(summaries, inputVector):\n",
    "\tprobabilities = {}\n",
    "\tfor classValue, classSummaries in summaries.items():\n",
    "\t\tprobabilities[classValue] = 1\n",
    "\t\tfor i in range(len(classSummaries)):\n",
    "\t\t\tmean, stdev = classSummaries[i]\n",
    "\t\t\tx = inputVector[i]\n",
    "\t\t\tprobabilities[classValue] *= calculateProbability(x, mean, stdev)\n",
    "\treturn probabilities\n",
    "\t\t\t\n",
    "def predict(summaries, inputVector):\n",
    "\tprobabilities = calculateClassProbabilities(summaries, inputVector)\n",
    "\tbestLabel, bestProb = None, -1\n",
    "\tfor classValue, probability in probabilities.items():\n",
    "\t\tif bestLabel is None or probability > bestProb:\n",
    "\t\t\tbestProb = probability\n",
    "\t\t\tbestLabel = classValue\n",
    "\treturn bestLabel\n",
    " \n",
    "def getPredictions(summaries, testSet):\n",
    "\tpredictions = []\n",
    "\tfor i in range(len(testSet)):\n",
    "\t\tresult = predict(summaries, testSet[i])\n",
    "\t\tpredictions.append(result)\n",
    "\treturn predictions\n",
    " \n",
    "def getAccuracy(testSet, predictions):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(testSet)):\n",
    "\t\tif testSet[i][-1] == predictions[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn (correct/float(len(testSet))) * 100.0\n",
    "\n",
    "def runBayes(i):\n",
    "    filename = 'sample-naive'+str(i)+'.csv'\n",
    "    splitRatio = 0.67\n",
    "    dataset = loadCsv(filename)\n",
    "\n",
    "    trainingSet, testSet = splitDataset(dataset, splitRatio)\n",
    "    summaries = summarizeByClass(trainingSet)\n",
    "    predictions = getPredictions(summaries, testSet)\n",
    "    accuracy = getAccuracy(testSet, predictions)\n",
    "    return accuracy\n",
    "\n",
    "x = runBayes(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Leitura dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sample-naive0.csv'\n",
    "splitRatio = 0.666\n",
    "dataset = loadCsv(filename)\n",
    "trainingSet, testSet = splitDataset(dataset, splitRatio)\n",
    "print('Split {0} rows into train={1} and test={2} rows'.format(len(dataset), len(trainingSet), len(testSet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Aprendendo as probabilidades com Gaussian Naïve Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = summarizeByClass(trainingSet)\n",
    "print('Summary by class value: {0}'.format(summaries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Predições para todos os exemplos do conjunto de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = getPredictions(summaries, testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Calculando a acurácia do modelo treinado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = getAccuracy(testSet, predictions)\n",
    "print('Accuracy: {0}%'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Random Decision Forest\n",
    "\n",
    "Floresta de Decisão Aleatória configura um método de aprendizado conjunto para classificação, regressão e outras tarefas que operam construindo uma multiplicidade de árvores de decisão no momento do treinamento e gerando a classe que é a moda das classes (classificação) ou predição média (regressão) das árvores individuais. Florestas de decisão aleatórias corrigem o hábito de overfitting de árvores de decisão para seu conjunto de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Gerando tabelas de contingência:\n",
    "\n",
    "Tabela de contingência tabula dados numa matriz que exibe a distribuição de frequência (multivariada) das variáveis, fornecem uma visualização básica da inter-relação entre variáveis e podem ajudar a encontrar interações entre elas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print('----------------------- Features Types --------------------------')\n",
    "df_sample = pd.read_csv('sample-tree0.csv', header=None)\n",
    "# data types of feature/attributes in the dataset\n",
    "print(df_sample.dtypes)\n",
    "\n",
    "print('\\n')\n",
    "df_sample.columns = ['Tuberculosis_or_Cancer','Dyspnea','X_Ray_Result','Bronchitis',\n",
    "                     'Lung_Cancer','Tuberculosis','Smoking','Visit_to_Asia']\n",
    "print('Class labels:', np.unique(df_sample['Tuberculosis_or_Cancer']))\n",
    "#df_sample.head()\n",
    "\n",
    "print('------------------------------  Contingency Table 1 -------------------------------')\n",
    "data_crosstab1 = pd.crosstab(df_sample['Tuberculosis_or_Cancer'], df_sample['Dyspnea'], margins = False)\n",
    "print(data_crosstab1)\n",
    "\n",
    "print('------------------------------  Contingency Table 2 -------------------------------')\n",
    "data_crosstab2 = pd.crosstab([df_sample.X_Ray_Result, df_sample.Dyspnea], df_sample.Tuberculosis_or_Cancer, margins = False) \n",
    "print(data_crosstab2)\n",
    "\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Dataset splitting into classes and features distributed in training and testing subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = df_sample.iloc[:, 1:].values, df_sample.iloc[:, 5].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.334, random_state=0)\n",
    "print('Class: ' + df_sample.columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Random Decision Forest Algorithm 1:\n",
    "\n",
    "Reuso do código da disponibilizado para a disicplina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feat_labels = df_sample.columns[1:] # class label é o indice 1 'Tuberculosis or Cancer'\n",
    "\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=10000,\n",
    "                                criterion='entropy',\n",
    "                                max_features='sqrt',\n",
    "                                random_state=0,\n",
    "                                n_jobs=-1)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30, \n",
    "                            feat_labels[indices[f]], \n",
    "                            importances[indices[f]]))\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.bar(range(X_train.shape[1]), \n",
    "        importances[indices],\n",
    "        color='lightblue', \n",
    "        align='center')\n",
    "\n",
    "plt.xticks(range(X_train.shape[1]), \n",
    "           feat_labels[indices], rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Random Decision Forest Algorithm 2:\n",
    "\n",
    "Referência externa, usando bootstrap aggregation (bagging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['0', '1', '1', '1', '0', '0', '0', '0'], ['1', '0', '0', '0', '1', '0', '0', '0'], ['0', '1', '1', '0', '0', '0', '1', '0'], ['0', '1', '0', '1', '0', '0', '1', '0'], ['0', '1', '0', '1', '0', '0', '1', '0'], ['1', '0', '0', '0', '0', '1', '0', '1'], ['0', '1', '1', '1', '0', '0', '1', '0'], ['0', '1', '0', '0', '0', '0', '0', '0'], ['0', '1', '0', '1', '0', '0', '0', '0'], ['0', '1', '0', '1', '0', '0', '1', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '1', '0', '0', '0', '0'], ['0', '1', '0', '1', '0', '0', '1', '0'], ['0', '1', '0', '1', '0', '0', '1', '0'], ['0', '1', '0', '0', '0', '0', '1', '0'], ['1', '0', '0', '0', '1', '0', '1', '0'], ['0', '1', '1', '1', '0', '0', '1', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '1', '0', '0', '1', '0'], ['1', '1', '0', '1', '1', '0', '1', '0'], ['0', '1', '1', '1', '0', '0', '1', '0'], ['0', '1', '0', '1', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '1', '0', '0', '1', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '1', '0', '1', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '1', '0', '1', '0', '0', '1', '0'], ['0', '0', '0', '0', '0', '0', '1', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '1', '0', '1', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '1', '0', '0', '0', '0', '0', '0'], ['0', '1', '0', '0', '0', '0', '0', '0'], ['0', '1', '0', '1', '0', '0', '0', '0'], ['1', '0', '0', '0', '0', '1', '0', '0'], ['0', '1', '1', '1', '0', '0', '1', '0'], ['0', '1', '0', '1', '0', '0', '1', '0'], ['0', '0', '0', '0', '0', '0', '1', '0'], ['0', '1', '0', '1', '0', '0', '1', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '1', '0', '0', '0', '0', '0', '0'], ['0', '1', '0', '0', '0', '0', '0', '0'], ['1', '1', '0', '1', '1', '0', '1', '0'], ['0', '1', '1', '0', '0', '0', '1', '0'], ['0', '1', '0', '1', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '1', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '1', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '1', '0', '1', '0', '0', '1', '0'], ['1', '0', '0', '0', '1', '0', '1', '0'], ['0', '1', '1', '1', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['1', '0', '0', '0', '0', '1', '1', '1'], ['0', '1', '1', '0', '0', '0', '1', '0'], ['1', '1', '0', '1', '0', '1', '1', '0'], ['1', '0', '1', '0', '1', '0', '0', '0'], ['1', '1', '1', '1', '0', '1', '0', '0'], ['0', '1', '1', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '1', '0', '1', '0', '0', '1', '0'], ['1', '1', '0', '1', '1', '0', '1', '0'], ['1', '1', '1', '0', '1', '1', '1', '0'], ['0', '0', '1', '1', '0', '0', '0', '0'], ['0', '1', '0', '1', '0', '0', '0', '0'], ['1', '1', '0', '1', '1', '1', '1', '1'], ['1', '0', '1', '1', '0', '1', '0', '0'], ['0', '0', '1', '0', '0', '0', '1', '0'], ['0', '1', '0', '1', '0', '0', '1', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '1', '0', '1', '0', '0', '1', '0'], ['0', '0', '0', '1', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '1', '0', '1', '0', '0', '1', '0'], ['0', '0', '0', '0', '0', '0', '1', '0'], ['0', '1', '0', '1', '0', '0', '1', '0'], ['1', '0', '0', '0', '0', '1', '0', '0'], ['0', '1', '1', '1', '0', '0', '0', '0'], ['1', '0', '0', '0', '0', '1', '1', '0'], ['0', '1', '1', '1', '0', '0', '1', '0'], ['0', '1', '1', '1', '0', '0', '0', '0'], ['0', '1', '0', '1', '0', '0', '0', '0'], ['1', '1', '0', '1', '0', '1', '1', '0'], ['0', '1', '1', '1', '0', '0', '1', '0'], ['1', '1', '0', '1', '0', '1', '0', '0'], ['0', '1', '1', '0', '0', '0', '1', '0'], ['0', '0', '0', '0', '0', '0', '1', '0'], ['1', '0', '0', '0', '1', '0', '1', '0'], ['1', '1', '0', '1', '0', '1', '1', '0'], ['0', '0', '1', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '1', '0'], ['0', '1', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '0', '0'], ['0', '0', '0', '0', '0', '0', '1', '0']]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Algorithm\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "        #print(dataset)\n",
    "    return dataset\n",
    "load_csv('sample-tree0.csv')\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    "\n",
    "# Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "\tleft, right = list(), list()\n",
    "\tfor row in dataset:\n",
    "\t\tif row[index] < value:\n",
    "\t\t\tleft.append(row)\n",
    "\t\telse:\n",
    "\t\t\tright.append(row)\n",
    "\treturn left, right\n",
    "\n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "\t# count all samples at split point\n",
    "\tn_instances = float(sum([len(group) for group in groups]))\n",
    "\t# sum weighted Gini index for each group\n",
    "\tgini = 0.0\n",
    "\tfor group in groups:\n",
    "\t\tsize = float(len(group))\n",
    "\t\t# avoid divide by zero\n",
    "\t\tif size == 0:\n",
    "\t\t\tcontinue\n",
    "\t\tscore = 0.0\n",
    "\t\t# score the group based on the score for each class\n",
    "\t\tfor class_val in classes:\n",
    "\t\t\tp = [row[-1] for row in group].count(class_val) / size\n",
    "\t\t\tscore += p * p\n",
    "\t\t# weight the group score by its relative size\n",
    "\t\tgini += (1.0 - score) * (size / n_instances)\n",
    "\treturn gini\n",
    "\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset, n_features):\n",
    "\tclass_values = list(set(row[-1] for row in dataset))\n",
    "\tb_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "\tfeatures = list()\n",
    "\twhile len(features) < n_features:\n",
    "\t\tindex = randrange(len(dataset[0])-1)\n",
    "\t\tif index not in features:\n",
    "\t\t\tfeatures.append(index)\n",
    "\tfor index in features:\n",
    "\t\tfor row in dataset:\n",
    "\t\t\tgroups = test_split(index, row[index], dataset)\n",
    "\t\t\tgini = gini_index(groups, class_values)\n",
    "\t\t\tif gini < b_score:\n",
    "\t\t\t\tb_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "\treturn {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
    "\n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "\toutcomes = [row[-1] for row in group]\n",
    "\treturn max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "\tleft, right = node['groups']\n",
    "\tdel(node['groups'])\n",
    "\t# check for a no split\n",
    "\tif not left or not right:\n",
    "\t\tnode['left'] = node['right'] = to_terminal(left + right)\n",
    "\t\treturn\n",
    "\t# check for max depth\n",
    "\tif depth >= max_depth:\n",
    "\t\tnode['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "\t\treturn\n",
    "\t# process left child\n",
    "\tif len(left) <= min_size:\n",
    "\t\tnode['left'] = to_terminal(left)\n",
    "\telse:\n",
    "\t\tnode['left'] = get_split(left, n_features)\n",
    "\t\tsplit(node['left'], max_depth, min_size, n_features, depth+1)\n",
    "\t# process right child\n",
    "\tif len(right) <= min_size:\n",
    "\t\tnode['right'] = to_terminal(right)\n",
    "\telse:\n",
    "\t\tnode['right'] = get_split(right, n_features)\n",
    "\t\tsplit(node['right'], max_depth, min_size, n_features, depth+1)\n",
    "\n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "\troot = get_split(train, n_features)\n",
    "\tsplit(root, max_depth, min_size, n_features, 1)\n",
    "\treturn root\n",
    "\n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "\tif row[node['index']] < node['value']:\n",
    "\t\tif isinstance(node['left'], dict):\n",
    "\t\t\treturn predict(node['left'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['left']\n",
    "\telse:\n",
    "\t\tif isinstance(node['right'], dict):\n",
    "\t\t\treturn predict(node['right'], row)\n",
    "\t\telse:\n",
    "\t\t\treturn node['right']\n",
    "\n",
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataset, ratio):\n",
    "\tsample = list()\n",
    "\tn_sample = round(len(dataset) * ratio)\n",
    "\twhile len(sample) < n_sample:\n",
    "\t\tindex = randrange(len(dataset))\n",
    "\t\tsample.append(dataset[index])\n",
    "\treturn sample\n",
    "\n",
    "# Make a prediction with a list of bagged trees\n",
    "def bagging_predict(trees, row):\n",
    "\tpredictions = [predict(tree, row) for tree in trees]\n",
    "\treturn max(set(predictions), key=predictions.count)\n",
    "\n",
    "# Random Forest Algorithm\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "\ttrees = list()\n",
    "\tfor i in range(n_trees):\n",
    "\t\tsample = subsample(train, sample_size)\n",
    "\t\ttree = build_tree(sample, max_depth, min_size, n_features)\n",
    "\t\ttrees.append(tree)\n",
    "\tpredictions = [bagging_predict(trees, row) for row in test]\n",
    "\treturn(predictions)\n",
    "\n",
    "# Test the random forest algorithm\n",
    "seed(2)\n",
    "# load and prepare data\n",
    "def runForest(i):\n",
    "    filename = 'sample-tree'+str(i)+'.csv'\n",
    "    dataset = load_csv(filename)\n",
    "    # convert string attributes to integers\n",
    "    for i in range(0, len(dataset[0])-1):\n",
    "        str_column_to_float(dataset, i)\n",
    "    # convert class column to integers\n",
    "    str_column_to_int(dataset, len(dataset[0])-1)\n",
    "    # evaluate algorithm\n",
    "    n_folds = 5\n",
    "    max_depth = 10\n",
    "    min_size = 1\n",
    "    sample_size = 1.0\n",
    "    n_features = int(sqrt(len(dataset[0])-1))\n",
    "    for n_trees in [1, 5, 10]:\n",
    "        scores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n",
    "        #print('Trees: %d' % n_trees)\n",
    "        #print('Scores: %s' % scores)\n",
    "        #print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "        return (sum(scores)/float(len(scores)))\n",
    "#runForest('sample-tree1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bateria de testes para Naïve Bayes\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bateria de testes para Random Decision Forests\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Discussão dos resultados\n",
    "\n",
    "• O quanto o tamanho do dataset gerado influencia na qualidade?<br>\n",
    "\n",
    "• Smoothing se faz necessário?<br>\n",
    "\n",
    "• Quem possui melhor comportamento: Naïve Bayes ou Random Decision Forest?<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floresta,bayes = [],[]\n",
    "for i in [1,2,3]:\n",
    "    floresta.append(runForest(i))\n",
    "    bayes.append(runBayes(i))\n",
    "plotGraphs(tuple(bayes), tuple(floresta),[10,100,1000,100000])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Considerações finais\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "Lauritzen, Steffen L. and David J. Spiegelhalter (1988) \"Local computations with probabilities on graphical structures and their application to expert systems\" in J. Royal Statistics Society B, 50(2), 157-194. Online em: www.eecis.udel.edu/~shatkay/Course/papers/Lauritzen1988.pdf\n",
    "\n",
    "RUSSEL, Stuart Jonathan; NORVIG, Peter. Artificial Intelligence: A Modern Approach. Third edition. [S.l.]:Pearson, 2009. Section 14.5.\n",
    "\n",
    "Brownlee, Jason (2017) \"How to Implement Random Forest From Scratch in Python\". Online em: www.machinelearningmastery.com/implement-random-forest-scratch-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Divisão de trabalho e checkpoints\n",
    "\n",
    "**• Airton:** Estudo do problema, foco nas regras do Game of Life e ações do problema (GoLaD), suporte na escrita do relatório, codificação e testes.<br>\n",
    "**• Eric:**   Estudo do problema, formulação inicial, estudo do Jupyter NB e do binder, formatação e escrita do relatório, codificação.<br>\n",
    "**• Igor:**   Estudo do problema, formulação final, adoção de solução inicial, codificação, submissão do bot inicial para testes.<br>\n",
    "**• Lucas:**  Estudo do problema, criação do repositório inicial, experienciar o game jogando e propor estratégias, codificação.<br>\n",
    "**• Hugo:**  Estudo do problema, criação do repositório inicial, experienciar o game jogando e propor estratégias, codificação.<br>\n",
    "**• Thiago:**  Estudo do problema, criação do repositório inicial, experienciar o game jogando e propor estratégias, codificação.\n",
    "<br>- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -<br>\n",
    "**12/12/2018:** Definição dos membros da equipe e do problema a ser resolvido.<br>\n",
    "**13/12/2018:** Primeiras impressões sobre o problema (estudo individual analítico sobre a mecânica e contexto histórico)<br>\n",
    "**18/12/2018:** Primeiras impressões sobre o problema (estudo individual analítico sobre a mecânica e contexto histórico)<br>\n",
    "**20/12/2018:** Hangout de debate sobre as implicações filosóficas e impacto científico-matemático do automato celular.<br>\n",
    "**21/12/2018:** Recesso declarado oficial, todos sumiram.<br>\n",
    "**07/01/2019:** Volta continua do recesso, os integrantes foram aparecendo aos poucos.<br>\n",
    "**08/01/2019:** Encontro presencial. Brainstorm. Definição da abordagem e técnica de IA para resolver o problema.<br>\n",
    "**09/01/2019:** Divisão do trabalho entre os membros da equipe.<br>\n",
    "**10/01/2019:** Tentativa de formulação do problema usando a abordagem de busca local populacional, com algoritmos genéticos.<br>\n",
    "**11/01/2019:** Tentativa de formulação do problema usando a abordagem de busca local populacional, com algoritmos genéticos.<br>\n",
    "**14/01/2019:** Redefinição da abordagem para busca com adversários, através da técnica de Expectiminimax e tentativa de formulação.<br>\n",
    "**14/01/2019:** Redefinição da abordagem, ainda em busca com adversários, porém através da técnica de Minimax e tentativa de formulação.<br>\n",
    "**15/01/2019:** Formulação.<br>\n",
    "**15/01/2019:** Estudo da plataforma Anaconda e Jupyter Notebook (tutoriais e minicursos virtuais).<br>\n",
    "**15/01/2019:** Inicialização de documentação no Jupyter Notebook do relatório do projeto.<br>\n",
    "**15/01/2019:** Formatação inicial do documento e descrição do problema de IA escolhido.<br>\n",
    "**16/01/2019:** Melhor compreensão da API da plataforma riddles.io para o Game of Life and Death e download do starter bot disponibilizado pela mesma.<br>\n",
    "**16/01/2019:** Teste de submissão do robô burro na plataforma riddles.io para competir com outros robôs.<br>\n",
    "**16/01/2019:** Instalação do PyCharm IDE e criação do repositório inicial no GitHub para codificação da solução adotada.<br>\n",
    "**16/01/2019:** Estudo da plataforma Binder para emular o servidor do Jupyter Notebook online (tutoriais).<br>\n",
    "**16/01/2019:** Escrita do relatório.<br>\n",
    "**17/01/2019:** Escrita do relatório.<br>\n",
    "**17/01/2019:** Estudo dos códigos disponibilizados para a disciplina.<br>\n",
    "**17/01/2019:** Implementação.<br>\n",
    "**18/01/2019:** Estudo dos códigos disponibilizados para a disciplina.<br>\n",
    "**18/01/2019:** Implementação.<br>\n",
    "**19/01/2019:** Estudo dos códigos disponibilizados para a disciplina.<br>\n",
    "**19/01/2019:** Implementação.<br>\n",
    "**19/01/2019:** Teste de submissão da solução adotada na plataforma riddles.io para competir com outros robôs.<br>\n",
    "**20/01/2019:** Estudo dos códigos disponibilizados para a disciplina.<br>\n",
    "**20/01/2019:** Implementação.<br>\n",
    "**20/01/2019:** Teste de submissão da solução adotada na plataforma riddles.io para competir com outros robôs.<br>\n",
    "**21/01/2019:** Implementação, testes finais e entrada.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Graficos mostrados direto do notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Definir estilo com template\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "def plotGraphs(bayes,forest,n):\n",
    "    # Dados\n",
    "    #bayes, bayes_std = (60, 70, 30), (2, 3, 4)\n",
    "    #forest, forest_std = (65, 68, 90), (3, 5, 2)\n",
    "\n",
    "    # Criar um vetor de índices (1,2,3...,n) com n elementos\n",
    "    ind = np.arange(len(bayes))\n",
    "    # Tamanho das barras (padrão)\n",
    "    width = 0.35\n",
    "    \n",
    "    n = tuple(n)\n",
    "    # Números para os pontos\n",
    "    #ticks = []\n",
    "    #a,b = 0,1\n",
    "    #for i in range(len(bayes)):\n",
    "    #    ticks.append(str(10**(a+b)))\n",
    "    #    a,b = b,a+b\n",
    "    #ticks = tuple(ticks)\n",
    "    # Criar gráfico\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Adicionar componentes ao gráfico\n",
    "    #rects1 = ax.bar(ind - width/2, bayes, width, yerr=bayes_std, label='Bayes')\n",
    "    rects1 = ax.bar(ind - width/2, bayes, width, label='Bayes')\n",
    "    #rects2 = ax.bar(ind + width/2, forest, width, yerr=forest_std, label='Forest')\n",
    "    rects2 = ax.bar(ind + width/2, forest,width, label='Forest')\n",
    "\n",
    "    # Títulos no grafico e nos eixos\n",
    "    ax.set_title('Precisão por volume de dados')\n",
    "    ax.set_ylabel('Precisão (%)')\n",
    "    ax.set_xlabel('Volume de dados')\n",
    "    #Número de pontos no eixo X\n",
    "    ax.set_xticks(ind) \n",
    "    # Número de pontos no eixo Y\n",
    "    ax.set_yticks(np.arange(0, 101, 10)) \n",
    "    # Editar os rótulos dos pontos do eixo\n",
    "    #ax.set_xticklabels(ticks)\n",
    "    ax.set_xticklabels(n)\n",
    "    # Adicionar legenda\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "bayes  = (60, 7, 30, 40,33)\n",
    "forest = (65, 68, 90, 50,88)\n",
    "plotGraphs(bayes,forest,[10,100,1000,10000,1000000])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
